# -*- coding: utf-8 -*-
"""data_flow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16k_oZTqws-6uCptCGIRy0ykzQ-ltVHtL
"""

!python --version

"""# AHoJ-DB Užklausa

https://apoholo.cz/db/search?pdb_ids=&uniprot_ids=&ligands=ADP%20ATP%20AMP%20GTP%20GDP%20FAD%20NAD%20NAP%20NDP%20HEM%20HEC%20PLM%20OLA%20MYR%20CIT%20CLA%20CHL%20BCL%20BCB&res_threshold=1.5

Rezoliucija: 1.5 Angstromų.
Ligandai: ADP ATP AMP GTP GDP FAD NAD NAP NDP HEM HEC PLM OLA MYR CIT CLA CHL BCL BCB

# Duomenų filtravimas
"""

import pandas as pd
import re

df = pd.read_csv('ahojdb_search_result.csv')
filtered_df = df[df[' num_apo_sites'] > 0]
filtered_df = filtered_df[filtered_df[' num_holo_sites'] > 0]
for index, row in filtered_df.iterrows():
    filtered_df.replace(row[' target_uniprot_ids'], re.sub('\\s.*', '', row[' target_uniprot_ids'].strip()), inplace=True)
filtered_df = filtered_df.drop_duplicates(subset=[' target_uniprot_ids'])
filtered_df.to_csv('filtered_ahojdb_search_result.csv')

"""# Paruošiame Apo and Holo porų duomenis"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

records = []

def parse_row(tr, prefix):
    cols = tr.find_all('td')
    d = {
        f'{prefix}_structure': cols[0].get_text(strip=True),
        f'{prefix}_aligned_chain': cols[1].get_text(strip=True),
        f'{prefix}_resolution': cols[2].get_text(strip=True),
        f'{prefix}_MBR': cols[3].get_text(strip=True),
        f'{prefix}_disorder_percent': cols[4].get_text(strip=True),
        f'{prefix}_RMSD': float(cols[5].get_text(strip=True)),
        f'{prefix}_pocket_RMSD': cols[6].get_text(strip=True),
        f'{prefix}_TM_score': cols[7].get_text(strip=True),
        f'{prefix}_SASA': cols[8].get_text(strip=True),
        f'{prefix}_volume': cols[9].get_text(strip=True),
    }
    if prefix == 'holo':
        d[f'{prefix}_ligands'] = cols[10].get_text(strip=True)
    return d

for entry_key in filtered_df['entry_key']:
    url = f'http://apoholo.cz/db/entry/{entry_key}'
    html = requests.get(url).content.decode()
    soup = BeautifulSoup(html, 'html.parser')

    # --- APO table ---
    apo_table = soup.find('span', class_='label', string='Found APO Sites:') \
                    .find_next('table')
    apo_rows = apo_table.find('tbody').find_all('tr')
    apo_records = [parse_row(r, 'apo') for r in apo_rows]
    apo_rmsd_vals = [r['apo_RMSD'] for r in apo_records]

    # --- HOLO table ---
    holo_table = soup.find('span', class_='label', string='Found HOLO Sites:') \
                     .find_next('table')
    holo_rows = holo_table.find('tbody').find_all('tr')
    holo_records = [parse_row(r, 'holo') for r in holo_rows]
    holo_rmsd_vals = [r['holo_RMSD'] for r in holo_records]

    # --- Compute per‐entry stats ---
    stats = {
        'entry_key': entry_key,
        'apo_count': len(apo_rmsd_vals),
        'apo_RMSD_min': min(apo_rmsd_vals),
        'apo_RMSD_max': max(apo_rmsd_vals),
        'apo_RMSD_avg': sum(apo_rmsd_vals) / len(apo_rmsd_vals),
        'holo_count': len(holo_rmsd_vals),
        'holo_RMSD_min': min(holo_rmsd_vals),
        'holo_RMSD_max': max(holo_rmsd_vals),
        'holo_RMSD_avg': sum(holo_rmsd_vals) / len(holo_rmsd_vals),
    }

    first_apo = apo_records[0]
    first_holo = holo_records[0]
    uniprot = filtered_df.loc[
        filtered_df['entry_key'] == entry_key, ' target_uniprot_ids'
    ].iloc[0]

    rec = {
        **stats,
        **first_apo,
        **first_holo,
        'uniprot_id': uniprot
    }
    records.append(rec)

df = pd.DataFrame(records)

print(df.head())

print("=== GLOBAL APO RMSD ===")
print("  min:", df['apo_RMSD_min'].min())
print("  max:", df['apo_RMSD_max'].max())
print("  avg:", df['apo_RMSD_avg'].mean())
print("  total count:", df['apo_count'].sum())

print("=== GLOBAL HOLO RMSD ===")
print("  min:", df['holo_RMSD_min'].min())
print("  max:", df['holo_RMSD_max'].max())
print("  avg:", df['holo_RMSD_avg'].mean())
print("  total count:", df['holo_count'].sum())

df.to_csv('apo_holo_pairs_with_RMSD_info.csv')

"""# Surenkame holo konformacijų struktūrų jonų ir ligandų informaciją."""

import requests
import pandas as pd

# Function to fetch PDB information using RCSB API
def fetch_pdb_info(pdb_id):
    url = f'https://data.rcsb.org/rest/v1/core/entry/{pdb_id[:4]}'
    response = requests.get(url)

    if response.status_code != 200:
        return {
            "PDB_ID": pdb_id,
            "Status": "Failed to fetch",
            "Conformers": None,
            "Ligands": None,
            "Ions": None,
            "Other_Ligands": None,
            "Other_Ions": None
        }

    data = response.json()

    if(pdb_id == '1kmv'):
        print('data', data)

    conformers = data.get('rcsb_entry_info', {}).get('polymer_entity_count_protein', None)

    # Define known ligands and ions
    known_ligands = {"BCB", "BCL", "CHL", "CLA", "CIT", "MYR", "OLA", "PLM",
                     "HEC", "HEM", "NDP", "NAP", "NAD", "FAD", "GDP", "GTP",
                     "AMP", "ATP", "ADP"}
    known_ions = {"MG", "ZN", "CL", "CA", "NA", "MN", "K", "FE", "CU", "CO"}

    ligands = []
    ions = []
    other_ligands = []
    other_ions = []

    # Extract ligands and ions from nonpolymer_entities
    nonpolymer_entities = data.get('nonpolymer_entities', [])

    for entity in nonpolymer_entities:
        chem_comp_name = entity.get('chem_comp', {}).get('id', '')

        if chem_comp_name in known_ligands:
            ligands.append(chem_comp_name)

        elif chem_comp_name in known_ions:
            ions.append(chem_comp_name)

        else:
            chem_comp_type = entity.get('rcsb_nonpolymer_entity', {}).get('chem_comp_type', '').upper()

            if 'ION' in chem_comp_type:
                other_ions.append(chem_comp_name)

            else:
                other_ligands.append(chem_comp_name)

    bound_components = data.get('rcsb_entry_info', {}).get('nonpolymer_bound_components', [])

    for component in bound_components:
        if component not in ligands and component not in ions:

            if component in known_ligands:
                ligands.append(component)

            elif component in known_ions:
                ions.append(component)

            else:
                other_ligands.append(component)

    return {
        "PDB_ID": pdb_id,
        "Status": "Success",
        "Conformers": conformers,
        "Ligands": ligands,
        "Ions": ions,
        "Other_Ligands": other_ligands,
        "Other_Ions": other_ions
    }

df = pd.read_csv('apo_holo_pairs_with_RMSD_info.csv')
results = []

for index, row in df.iterrows():
    holo_pdb = row['holo_structure'][:4]
    apo_pdb = row['apo_structure'][:4]
    print(f"Fetching information for Holo PDB ID: {holo_pdb}")
    result = fetch_pdb_info(holo_pdb)
    result["Apo_PDB_ID"] = apo_pdb
    result["Holo_PDB_ID"] = holo_pdb
    results.append(result)

results_df = pd.DataFrame(results)

# Filter results: keep only rows where Other_Ligands are empty
filtered_results_df = results_df[(results_df['Other_Ligands'].apply(len) == 0)]

filtered_results_df = filtered_results_df[["Apo_PDB_ID", "Holo_PDB_ID", "Ligands", "Ions"]]

# Display the filtered results
print(filtered_results_df)
filtered_results_df.to_csv("apo_holo_pair_ligand_and_ion_data.csv", index=False)

!rm -rf /content/cif_files

"""# Atsisiunčiame struktūrų CIF failus.

Peržiūrime struktūras rankiniu būdu, užtikrindami jonų egzistvimą holo struktūrose bei panašumą su apo struktūromis.



"""

import os
import requests
import pandas as pd

# Function to download CIF files
def download_cif(pdb_id, output_dir):
    url = f'https://files.rcsb.org/download/{pdb_id}.cif'
    response = requests.get(url)

    if response.status_code == 200:
        filepath = os.path.join(output_dir, f"{pdb_id}.cif")
        with open(filepath, 'wb') as file:
            file.write(response.content)
        print(f"Downloaded: {pdb_id}.cif")
        return True
    else:
        print(f"Failed to download: {pdb_id}.cif")
        return False

# Load the DataFrame from the previous script's filtered results
input_csv = "reviewed_apo_holo_pair_ligand_and_ion_data.tsv"
output_dir = "cif_files"
os.makedirs(output_dir, exist_ok=True)

# Read the filtered results CSV
filtered_results_df = pd.read_csv(input_csv, sep='\t')

# Download CIF files for both apo and holo IDs
for index, row in filtered_results_df.iterrows():
    apo = row['Apo_PDB_ID'][:4]
    holo = row['Holo_PDB_ID'][:4]
    print(f"Processing pair: Apo - {apo}, Holo - {holo}")
    download_cif(apo, output_dir)  # Download CIF for apo
    download_cif(holo, output_dir)  # Download CIF for holo

rmsd_df = pd.read_csv('apo_holo_pairs_with_RMSD_info.csv')
reviewed_df = pd.read_csv('reviewed_apo_holo_pair_ligand_and_ion_data.tsv', sep='\t')

reviewed_ids = set(reviewed_df['PDB_ID'].str.upper())

rmsd_df['holo_id'] = rmsd_df['holo_structure'].str[:4].str.upper()

fi_df = rmsd_df[rmsd_df['holo_id'].isin(reviewed_ids)].copy()

fi_df = fi_df[['holo_id', 'apo_count', 'apo_RMSD_min', 'apo_RMSD_max', 'apo_RMSD_avg', 'holo_count', 'holo_RMSD_min', 'holo_RMSD_max', 'holo_RMSD_avg']]
print(fi_df)

fi_df.to_csv('reviewed_RMSD_info.csv', index=False)

!rm -rf /content/structures_from_pdb.zip
!zip -r structures_from_pdb.zip /content/cif_files/

!pip install gemmi

"""# Skaičiuojame ligandų ir jonų kiekį holo struktūrose"""

import gemmi
from collections import Counter
import os
import pandas as pd
import re

# Define known ligands and ions
KNOWN_LIGANDS = {"BCB", "BCL", "CHL", "CLA", "CIT", "MYR", "OLA", "PLM", "HEC", "HEM", "NDP", "NAP", "NAD", "FAD", "GDP", "GTP", "AMP", "ATP", "ADP"}
KNOWN_LIGAND_MAP = {
    "BACTERIOCHLOROPHYLL B": "BCB",
    "BACTERIOCHLOROPHYLL": "BCL",
    "CHLOROPHYLL A": "CHL",
    "CHLOROPHYLL B": "CLA",
    "CITRIC ACID": "CIT",
    "MYRISTIC ACID": "MYR",
    "OLEIC ACID": "OLA",
    "PALMITIC ACID": "PLM",
    "HEMATOPOIETIC ENZYME COMPLEX": "HEC",
    "HEME C": "HEC",
    "HEME": "HEM",
    "PROTOPORPHYRIN IX CONTAINING FE": "HEM",
    "NADP NICOTINAMIDE-ADENINE-DINUCLEOTIDE PHOSPHATE": "NDP",
    "NADPH DIHYDRO-NICOTINAMIDE-ADENINE-DINUCLEOTIDE PHOSPHATE": "NDP",
    "NICOTINAMIDE-ADENINE-DINUCLEOTIDE": "NAD",
    "FLAVIN-ADENINE DINUCLEOTIDE": "FAD",
    "GUANOSINE-5'-DIPHOSPHATE": "GDP",
    "GUANOSINE-5'-TRIPHOSPHATE": "GTP",
    "ADENOSINE MONOPHOSPHATE": "AMP",
    "ADENOSINE-5'-TRIPHOSPHATE": "ATP",
    "ADENOSINE-5'-DIPHOSPHATE": "ADP",
}

KNOWN_IONS = {"MG", "ZN", "CL", "CA", "NA", "MN", "K", "FE", "CU", "CO"}
KNOWN_ION_MAP = {
    "MAGNESIUM ION": "MG",
    "ZINC ION": "ZN",
    "CHLORIDE ION": "CL",
    "CALCIUM ION": "CA",
    "SODIUM ION": "NA",
    "MANGANESE ION": "MN",
    "POTASSIUM ION": "K",
    "FE ION": "FE",
    "COPPER ION": "CU",
    "COBALT ION": "CO",
}

def parse_cif(file_path):
    doc = gemmi.cif.read_file(file_path)
    block = doc.sole_block()
    entity = []
    count = []
    for element in block.find_loop("_entity.pdbx_number_of_molecules"):
        count.append(element)
    for element in block.find_loop("_entity.pdbx_description"):
        entity.append(element)

    entity_dict = dict(zip(entity, count))
    filtered_ion_dict = dict()
    filtered_ligand_dict = dict()

    for key, value in entity_dict.items():
        key = key.strip("'")
        key = re.sub('\\(.*\\) ', '', key)
        key = key.strip("\"")

        if key in KNOWN_LIGAND_MAP:
            filtered_ligand_dict[KNOWN_LIGAND_MAP[key]] = value

        if key in KNOWN_ION_MAP:
            filtered_ion_dict[KNOWN_ION_MAP[key]] = value

    return filtered_ligand_dict, filtered_ion_dict

# Main function to process CIF files in a directory
def process_cif_directory(directory, holo_ids):
    results = []
    for file_name in os.listdir(directory):
        if file_name.endswith(".cif"):
            file_path = os.path.join(directory, file_name)

            if file_name[:4] not in holo_ids:
                continue

            print(f"Processing {file_name}...")
            filtered_ligand_dict, filtered_ion_dict  = parse_cif(file_path)

            results.append({
                "Holo_id": file_name[0:4],
                "Ligands": list(filtered_ligand_dict.keys()),
                "Ligand_count": list(filtered_ligand_dict.values()),
                "Ions": list(filtered_ion_dict.keys()),
                "Ions_count": list(filtered_ion_dict.values())
            })

    # Convert to DataFrame
    results_df = pd.DataFrame(results)
    return results_df

df = pd.read_csv("reviewed_apo_holo_pair_ligand_and_ion_data.tsv", sep='\t')
holo_ids = list()

for id in df["Holo_PDB_ID"]:
    holo_ids.append(id)

# Path to directory containing CIF files
cif_directory = "/content/cif_files"

# Process CIF files and get results
cif_results_df = process_cif_directory(cif_directory, holo_ids)

cif_results_df.to_csv("ligand_and_ion_counts.csv", index=False)
print(cif_results_df)

"""# Suporuojame struktūras su uniprot identifikatoriumi"""

reviewed_holo_id_df = pd.read_csv("/content/reviewed_apo_holo_pair_ligand_and_ion_data.tsv", sep='\t')
uniprot_id_df = pd.read_csv("/content/apo_holo_pairs_with_RMSD_info.csv")

for index, row in uniprot_id_df.iterrows():
    uniprot_id_df.replace(row['holo_structure'], row['holo_structure'][0:4], inplace=True)
    uniprot_id_df.replace(row['uniprot_id'], re.sub('\\s.*', '', row['uniprot_id'].strip()), inplace=True)

print(uniprot_id_df['holo_structure'], reviewed_holo_id_df['Holo_PDB_ID'])

merged_df = uniprot_id_df.merge(
    reviewed_holo_id_df[['Holo_PDB_ID']],
    left_on='holo_structure',
    right_on='Holo_PDB_ID',
    how='inner'
)

reviewed_uniprot_ids = merged_df['uniprot_id'].tolist()

merged_df[['uniprot_id', 'Holo_PDB_ID']].to_csv('uniprot_holo_pdb_list.tsv', sep='\t', index=False)

!rm -rf /content/uniprot_entries
!rm -rf /content/uniprot_entries.zip

"""# Atsisiunčiame struktūrų sekas FASTA formatu naudojantis uniprot identifikatoriais ir API"""

import os
import requests

def download_fasta_sequences(uniprot_ids, output_dir):
    base_url = "https://rest.uniprot.org/uniprotkb/"
    headers = {"Accept": "text/x-fasta"}

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    for uniprot_id in uniprot_ids:
        uniprot_id = uniprot_id.strip()
        print(f"Fetching FASTA sequence for UniProt ID: {uniprot_id}...")
        url = f"{base_url}{uniprot_id}.fasta"
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            # Save the FASTA sequence to a file named `uniprot_id.fasta`
            file_path = os.path.join(output_dir, f"{uniprot_id}.fasta")
            with open(file_path, "w") as fasta_file:
                fasta_file.write(response.text)
            print(f"Sequence saved to {file_path}")
        else:
            print(f"Failed to fetch FASTA sequence for {uniprot_id}. Status code: {response.status_code}")

# List of UniProt IDs to download
uniprot_holo_pdb_list = pd.read_csv("/content/uniprot_holo_pdb_list.tsv", sep='\t')

reviewed_uniprot_ids = uniprot_holo_pdb_list['uniprot_id'].tolist()
# Output directory
output_dir = "/content/uniprot_entries/"
os.makedirs(output_dir, exist_ok=True)
print(reviewed_uniprot_ids)
# Download the sequences
download_fasta_sequences(reviewed_uniprot_ids, output_dir)

!zip -r uniprot_entries.zip /content/uniprot_entries/

!pip install biopython

"""# Sugeneruojame JSON darbų failus AlphaFold3 įrankiui"""

import os
import json
import pandas as pd
from Bio import SeqIO
import re
# Directory containing Uniprot FASTA files
fasta_dir = "uniprot_entries"

# Function to read the sequence from a Uniprot FASTA file
def get_sequence(uniprot_id):
    fasta_file = os.path.join(fasta_dir, f"{uniprot_id}.fasta")
    if not os.path.exists(fasta_file):
        print(f"FASTA file not found for Uniprot ID: {uniprot_id}")
        return None
    for record in SeqIO.parse(fasta_file, "fasta"):
        return str(record.seq)
    return None

# Function to create AlphaFold job JSON
def create_alphafold_job(job_name, sequence, ligands=None, ions=None):
    sequences = [
        {
            "proteinChain": {
                "sequence": sequence,
                "count": 1
            }
        }
    ]

    if ligands:
        for ligand, count in ligands.items():
            sequences.append({
                "ligand": {
                    "ligand": f"CCD_{ligand}",
                    "count": count
                }
            })

    if ions:
        for ion, count in ions.items():
            sequences.append({
                "ion": {
                    "ion": ion,
                    "count": count
                }
            })

    job_data = {
        "name": job_name,
        "modelSeeds": [],
        "sequences": sequences
    }

    return job_data

# Main script
if __name__ == "__main__":
    # Load data files
    input_tsv = "/content/reviewed_apo_holo_pair_ligand_and_ion_data.tsv"
    uniprot_holo_pdb_df = pd.read_csv("/content/uniprot_holo_pdb_list.tsv", sep='\t')
    ligand_ion_file = "/content/ligand_and_ion_counts.csv"
    output_file = "alphafold_jobs.json"

    # Read the main data file
    data = pd.read_csv(input_tsv, sep='\t')

    # Read ligand and ion counts
    counts_data = pd.read_csv(ligand_ion_file)
    ligand_counts = counts_data.set_index('Holo_id')['Ligand_count'].to_dict()
    ion_counts = counts_data.set_index('Holo_id')['Ions_count'].to_dict()
    all_jobs = []

    for _, row in data.iterrows():
        apo_id = row['Apo_PDB_ID']
        holo_id = row['Holo_PDB_ID']
        ligands = eval(row['Ligands']) if 'Ligands' in row and pd.notna(row['Ligands']) else []
        ions = eval(row['Ions']) if 'Ions' in row and pd.notna(row['Ions']) else []
        # Get Uniprot ID from mapping
        uniprot_id = uniprot_holo_pdb_df.loc[uniprot_holo_pdb_df['Holo_PDB_ID']== holo_id[0:4], 'uniprot_id'].iloc[0]
        print(uniprot_id, holo_id)
        if not uniprot_id:
            print(f"No Uniprot ID found for {holo_id}")
            continue

        # Get sequences for apo and holo structures
        apo_sequence = get_sequence(uniprot_id)
        holo_sequence = get_sequence(uniprot_id)

        if apo_sequence:
            apo_job = create_alphafold_job(job_name=f'{apo_id}_apo', sequence=apo_sequence)
            all_jobs.append(apo_job)
            print(f"Added AlphaFold job for apo: {apo_id}")

        if holo_sequence:
            ligand_dict = {}
            for i, ion in enumerate(ligands):
              counts = list(map(int, re.findall(r"\d+", ligand_counts.get(holo_id[0:4], 1))))
              ligand_dict[ion] = counts[i]

            ion_dict = {}
            for i, ion in enumerate(ions):
              counts = list(map(int, re.findall(r"\d+", ion_counts.get(holo_id[0:4], 1))))
              ion_dict[ion] = counts[i]
            holo_job = create_alphafold_job(job_name=f'{holo_id}_holo', sequence=holo_sequence, ligands=ligand_dict, ions=ion_dict)
            all_jobs.append(holo_job)
            print(f"Added AlphaFold job for holo: {holo_id}")
    print(len(all_jobs))

    def save_jobs_in_parts(all_jobs, output_file_base, max_jobs=100):
        total_jobs = len(all_jobs)
        num_parts = (total_jobs + max_jobs - 1) // max_jobs  # Ceiling division

        for i in range(num_parts):
            start = i * max_jobs
            end = min((i + 1) * max_jobs, total_jobs)
            part_jobs = all_jobs[start:end]

            filename = f"{output_file_base}_part_{i + 1}.json"
            with open(filename, "w") as outfile:
                json.dump(part_jobs, outfile, indent=4)
            print(f"Saved {len(part_jobs)} jobs to {filename}")

    output_file_base = "AlphaFold_jobs"
    save_jobs_in_parts(all_jobs, output_file_base, max_jobs=30)

"""# Išpakuojame prognozuotas struktūras"""

!mkdir /content/all_folds
!unzip /content/folds_2025_04_16_12_36_part_1.zip -d /content/all_folds
!unzip /content/folds_2025_04_16_12_45_part_2.zip -d /content/all_folds
!unzip /content/folds_2025_04_17_08_00_part_3.zip -d /content/all_folds
!unzip /content/folds_2025_04_17_07_46_part_4.zip -d /content/all_folds
!unzip /content/folds_2025_04_17_08_13_part_5.zip -d /content/all_folds
!unzip /content/folds_2025_04_17_09_40_part_6.zip -d /content/all_folds

"""# Skaičiuojame plDDt įverčius"""

import os
import json
import re

# Directory containing your JSON files
data_dir = "all_folds"

# Output TSV file
output_file = "plddt_results.tsv"

# 1. Discover all fold numbers across all folders
fold_numbers = set()
pattern = re.compile(r".*full_data_(\d+)\.json$")
for folder in os.listdir(data_dir):
    folder_path = os.path.join(data_dir, folder)
    if not os.path.isdir(folder_path):
        continue
    for filename in os.listdir(folder_path):
        m = pattern.match(filename)
        print(filename, m)
        if m:
            fold_numbers.add(int(m.group(1)))

# Sort so columns come out in numeric order
fold_numbers = sorted(fold_numbers)

# 2. Write header
with open(output_file, "w") as outfile:
    header = ["PDB_NAME"] + [f"PLDDT_{n}" for n in fold_numbers]
    outfile.write("\t".join(header) + "\n")

    # 3. Process each folder and collect averages
    for folder in os.listdir(data_dir):
        folder_path = os.path.join(data_dir, folder)
        if not os.path.isdir(folder_path):
            continue

        # compute avg pLDDT per fold in this folder
        avg_by_fold = {}
        for filename in os.listdir(folder_path):
            m = pattern.match(filename)
            if not m:
                continue
            fold_idx = int(m.group(1))
            json_path = os.path.join(folder_path, filename)
            with open(json_path, "r") as f:
                data = json.load(f)
            atom_plddts = data.get("atom_plddts", [])
            if atom_plddts:
                avg_plddt = sum(atom_plddts) / len(atom_plddts)
                avg_by_fold[fold_idx] = avg_plddt

        # build row: PDB name (uppercase) + values or blanks
        row = [folder.upper()]
        for n in fold_numbers:
            if n in avg_by_fold:
                row.append(f"{avg_by_fold[n]:.2f}")
            else:
                row.append("")  # or use "NA" if you prefer
        outfile.write("\t".join(row) + "\n")

print(f"pLDDT calculations completed. Results saved in {output_file}.")

"""# Skaičiuojame RMSD įverčius"""

# Commented out IPython magic to ensure Python compatibility.
from IPython.utils import io
import tqdm.notebook
import os
"""The PyMOL installation is done inside two nested context managers. This approach
was inspired by Dr. Christopher Schlick's (of the Phenix group at
Lawrence Berkeley National Laboratory) method for installing cctbx
in a Colab Notebook. He presented his work on September 1, 2021 at the IUCr
Crystallographic Computing School. I adapted Chris's approach here. It replaces my first approach
that requires seven steps. My approach was presentated at the SciPy2021 conference
in July 2021 and published in the
[proceedings](http://conference.scipy.org/proceedings/scipy2021/blaine_mooers.html).
The new approach is easier for beginners to use. The old approach is easier to debug
and could be used as a back-up approach.

Thank you to Professor David Oppenheimer of the University of Florida for suggesting the use mamba and of Open Source PyMOL.
"""
total = 100
with tqdm.notebook.tqdm(total=total) as pbar:
    with io.capture_output() as captured:

        !pip install -q condacolab
        import condacolab
        condacolab.install()
        pbar.update(10)

        import sys
        sys.path.append('/usr/local/lib/python3.10/site-packages/')
        pbar.update(20)

        # Install PyMOL
#         %shell mamba install pymol-open-source --yes

        pbar.update(100)

!unzip /content/structures_from_pdb.zip -d /content/cif_files

from pymol import cmd
# from IPython.display import Image
import pandas as pd
df = pd.read_csv("/content/reviewed_apo_holo_pair_ligand_and_ion_data.tsv", sep='\t')
pairs = []
for index, row in df.iterrows():
        apo = row['Apo_PDB_ID']
        holo = row['Holo_PDB_ID']
        pairs.append([apo, holo])
RMSD_map = dict()
for pair in pairs:
  cmd.reinitialize()
  cmd.load(f"/content/cif_files/content/cif_files/{pair[0][0:4]}.cif", "str1")
  cmd.load(f"/content/cif_files/content/cif_files/{pair[1][0:4]}.cif", "str2")
  RMSD = cmd.align("str2", "str1")[0]  # Align structure2 to structure1
  RMSD_map[f'{pair[0][0:4]}_{pair[1][0:4]}'] = RMSD
  print(f"RMSD between {pair[0]} and {pair[1]}: {RMSD}")

df['APO_vs_HOLO'] = RMSD_map.values()
print(df)

# APO X HOLO_ALPHA
new_pairs = []
for pair in pairs:
  alpha_model = pair[1] + '_holo'
  # print(alpha_model)
  new_pairs.append([pair[0], alpha_model])
RMSD_map = dict()
pairs_w_model = dict()
model_w_pdb_ids = pd.DataFrame(columns=['apo_pdb_id', 'holo_pdb_id', 'alphafold_model_id'])
model = 0
for pair in new_pairs:
  RMSD = 0
  for i in range(0, 5):
    cmd.reinitialize()
    cmd.load(f"/content/cif_files/content/cif_files/{pair[0][0:4]}.cif", "str1")
    cmd.load(f"/content/all_folds/{pair[1]}/fold_{pair[1]}_model_{i}.cif", "str2")
    RMSD_new = cmd.align("str2", "str1")[0]  # Align structure2 to structure1
    if RMSD < RMSD_new:
      RMSD = RMSD_new
      model = i
  RMSD_map[f'{pair[0][0:4]}_{pair[1][0:4]}'] = RMSD
  new_row = {
    'apo_pdb_id':          pair[0],
    'holo_pdb_id':         pair[1][:4],
    'alphafold_model_id':  model
  }
  model_w_pdb_ids.loc[len(model_w_pdb_ids)] = new_row
#   model_w_pdb_ids.append({'apo_pdb_id': pair[0], 'holo_pdb_id': pair[1][:4], 'alphafold_model_id': model}, ignore_index=True)
  pairs_w_model[pair[0]] = model
  print(f"RMSD between {pair[0]} and {pair[1][0:4]}_alpha: {RMSD}")

df['APO_vs_HOLO_ALPHA'] = RMSD_map.values()
print(df)
print(model_w_pdb_ids)
model_w_pdb_ids.to_csv('model_w_pdb_ids.csv', index=False)

# HOLO X APO_ALPHA
new_pairs = []
for pair in pairs:
  alpha_model = pair[0] + '_apo'
  # print(alpha_model)
  new_pairs.append([pair[1], alpha_model, pairs_w_model[pair[0]]])
RMSD_map = dict()
for pair in new_pairs:
  # RMSD = 100
  cmd.reinitialize()
  cmd.load(f"/content/cif_files/content/cif_files/{pair[0]}.cif", "str1")
  cmd.load(f"/content/all_folds/{pair[1]}/fold_{pair[1]}_model_{pair[2]}.cif", "str2")
  RMSD = cmd.align("str2", "str1")[0]  # Align structure2 to structure1

      # print(f"Model: {i}")
  RMSD_map[f'{pair[0]}_{pair[1][0:4]}'] = RMSD
  print(f"RMSD between {pair[0]} and {pair[1][0:4]}_alpha: {RMSD}")

df = pd.read_csv("/content/RMSD_results.csv", sep=',')
df['HOLO_vs_APO_ALPHA'] = RMSD_map.values()
print(df)
df.to_csv('RMSD_results.csv', index=False)

# APO X APO_ALPHA
new_pairs = []
for pair in pairs:
  alpha_model = pair[0] + '_apo'
  # print(alpha_model)
  new_pairs.append([pair[0], alpha_model, pairs_w_model[pair[0]]])
RMSD_map = dict()
for pair in new_pairs:
  # RMSD = 100
  cmd.reinitialize()
  cmd.load(f"/content/cif_files/content/cif_files/{pair[0]}.cif", "str1")
  cmd.load(f"/content/all_folds/{pair[1]}/fold_{pair[1]}_model_{pair[2]}.cif", "str2")
  RMSD = cmd.align("str2", "str1")[0]  # Align structure2 to structure1

      # print(f"Model: {i}")
  RMSD_map[f'{pair[0]}_{pair[1][0:4]}'] = RMSD
  print(f"RMSD between {pair[0]} and {pair[1][0:4]}_alpha: {RMSD}")

df['APO_vs_APO_ALPHA'] = RMSD_map.values()
print(df)

# HOLO X HOLO_ALPHA
new_pairs = []
for pair in pairs:
  alpha_model = pair[1] + '_holo'
  # print(alpha_model)
  new_pairs.append([pair[1], alpha_model, pairs_w_model[pair[0]]])
RMSD_map = dict()
for pair in new_pairs:
  # RMSD = 100
  cmd.reinitialize()
  cmd.load(f"/content/cif_files/content/cif_files/{pair[0]}.cif", "str1")
  cmd.load(f"/content/all_folds/{pair[1]}/fold_{pair[1]}_model_{pair[2]}.cif", "str2")
  RMSD = cmd.align("str2", "str1")[0]  # Align structure2 to structure1

      # print(f"Model: {i}")
  RMSD_map[f'{pair[0]}_{pair[1][0:4]}'] = RMSD
  print(f"RMSD between {pair[0]} and {pair[1][0:4]}_alpha: {RMSD}")

df['HOLO_vs_HOLO_ALPHA'] = RMSD_map.values()
print(df)

# APO_ALPHA X HOLO_ALPHA
new_pairs = []
for pair in pairs:
  alpha_model_apo = pair[0] + '_apo'
  alpha_model_holo = pair[1] + '_holo'
  # print(alpha_model)
  new_pairs.append([alpha_model_apo, alpha_model_holo, pairs_w_model[pair[0]]])
RMSD_map = dict()
for pair in new_pairs:
  # RMSD = 100
  cmd.reinitialize()
  cmd.load(f"/content/all_folds/{pair[0]}/fold_{pair[0]}_model_{pair[2]}.cif", "str1")
  cmd.load(f"/content/all_folds/{pair[1]}/fold_{pair[1]}_model_{pair[2]}.cif", "str2")
  RMSD = cmd.align("str2", "str1")[0]  # Align structure2 to structure1

      # print(f"Model: {i}")
  RMSD_map[f'{pair[0][0:4]}_{pair[1][0:4]}'] = RMSD
  print(f"RMSD between {pair[0][0:4]}_apo_alpha and {pair[1][0:4]}_holo_alpha: {RMSD}")

df['APO_ALPHA_vs_HOLO_ALPHA'] = RMSD_map.values()
print(df)

df.to_csv('RMSD_results.csv', index=False)